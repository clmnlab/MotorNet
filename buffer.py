import numpy as np
import torch
import torch.nn as nn

# ======================================================================================
# 1. í…ŒìŠ¤íŠ¸ ëŒ€ìƒ ë²„í¼ ì½”ë“œ (Canvasì— ìˆëŠ” ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜´)
# ======================================================================================
class RolloutBuffer:
    """
    ìˆœí™˜ ì‹ ê²½ë§(RNN/GRU) í•™ìŠµì„ ìœ„í•œ ë¡¤ì•„ì›ƒ ë²„í¼ (ë°°ì¹˜ ë°ì´í„° ì§€ì›).
    ë°ì´í„°ì˜ ì‹œê°„ì  ìˆœì„œë¥¼ ìœ ì§€í•˜ê³ , í•™ìŠµ ì‹œ ì—°ì†ëœ ì‹œí€€ìŠ¤ ë‹¨ìœ„ì˜ ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    def __init__(self,
                 obs_dim: int,
                 action_dim: int,
                 hidden_dim: int,
                 batch_size: int,              # ë¡¤ì•„ì›ƒ ì‹œ í•œ ìŠ¤í…ì— ì²˜ë¦¬í•˜ëŠ” ë°ì´í„°ì˜ ë°°ì¹˜ í¬ê¸°
                 sequence_length: int = 100,
                 n_steps: int = 100,
                 gamma: float = 0.99,
                 gae_lambda: float = 0.95,
                 mini_batch_size: int = 16,    # [ì˜ë¯¸ ë³€ê²½] í•œ ë¯¸ë‹ˆë°°ì¹˜ì— í¬í•¨ë  'ì‹œí€€ìŠ¤ì˜ ê°œìˆ˜'
                 device: torch.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")):

        self.n_steps = n_steps
        self.batch_size = batch_size
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.sequence_length = sequence_length
        self.mini_batch_size = mini_batch_size
        self.device = device
        self.gamma = gamma
        self.gae_lambda = gae_lambda

        self.reset()

    def reset(self):
        self.states = np.zeros((self.n_steps, self.batch_size, self.obs_dim), dtype=np.float32)
        # CartPoleì€ ì´ì‚° í–‰ë™ ê³µê°„ì´ë¯€ë¡œ actionì€ ìŠ¤ì¹¼ë¼, raw_actionì€ ë¡œì§“(2)
        self.raw_actions = np.zeros((self.n_steps, self.batch_size, self.action_dim), dtype=np.float32)
        self.actions = np.zeros((self.n_steps, self.batch_size, self.action_dim), dtype=np.float32)
        self.rewards = np.zeros((self.n_steps, self.batch_size), dtype=np.float32)
        self.dones = np.zeros((self.n_steps, self.batch_size), dtype=np.float32)
        self.log_probs = np.zeros((self.n_steps, self.batch_size), dtype=np.float32)
        self.values = np.zeros((self.n_steps, self.batch_size), dtype=np.float32)
        self.advantages = np.zeros((self.n_steps, self.batch_size), dtype=np.float32)
        self.returns = np.zeros((self.n_steps, self.batch_size), dtype=np.float32)
        self.hidden_states = np.zeros((self.n_steps, self.batch_size, self.hidden_dim), dtype=np.float32)
        self.ptr = 0
        self.full = False

    def add(self, state, raw_action, action, reward, done, log_prob, value, hidden_state):
        if self.ptr >= self.n_steps:
            self.full = True
            return

        self.states[self.ptr] = state
        self.raw_actions[self.ptr] = raw_action
        self.actions[self.ptr] = action
        self.rewards[self.ptr] = reward
        self.dones[self.ptr] = done
        self.log_probs[self.ptr] = log_prob
        self.values[self.ptr] = value.flatten()
        self.hidden_states[self.ptr] = hidden_state.detach().cpu().numpy().reshape(self.batch_size, self.hidden_dim)
        self.ptr = (self.ptr + 1)

    def compute_returns_and_advantages(self, last_values, last_dones):
        """GAE ê³„ì‚°. last_values/donesëŠ” (batch_size,) í˜•íƒœì˜ ë²¡í„°ì…ë‹ˆë‹¤."""
        # ğŸ”” [ìˆ˜ì •] ì…ë ¥ê°’ì´ ìŠ¤ì¹¼ë¼ì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ë°°ì—´ë¡œ ë³€í™˜í•˜ëŠ” ë¡œì§ ì¶”ê°€
        last_values = np.asarray(last_values).flatten()
        last_dones = np.asarray(last_dones).flatten()      
        last_gae_lam = 0
        for t in reversed(range(self.n_steps)):
            if t == self.n_steps - 1:
                next_non_terminal = 1.0 - last_dones
                next_values = last_values
            else:
                next_non_terminal = 1.0 - self.dones[t + 1]
                next_values = self.values[t + 1]

            delta = self.rewards[t] + self.gamma * next_values * next_non_terminal - self.values[t]
            last_gae_lam = delta + self.gamma * self.gae_lambda * next_non_terminal * last_gae_lam
            self.advantages[t] = last_gae_lam
        self.returns = self.advantages + self.values

    def __iter__(self):
        all_data = {
            'states': torch.tensor(self.states, dtype=torch.float32).swapaxes(0, 1),
            'raw_actions': torch.tensor(self.raw_actions, dtype=torch.float32).swapaxes(0, 1),
            'actions': torch.tensor(self.actions, dtype=torch.float32).swapaxes(0, 1),
            'log_probs': torch.tensor(self.log_probs, dtype=torch.float32).swapaxes(0, 1),
            'advantages': torch.tensor(self.advantages, dtype=torch.float32).swapaxes(0, 1),
            'returns': torch.tensor(self.returns, dtype=torch.float32).swapaxes(0, 1),
            'hidden_states': torch.tensor(self.hidden_states, dtype=torch.float32).swapaxes(0, 1),
        }

        n_sequences_per_stream = self.n_steps // self.sequence_length
        num_total_sequences = self.batch_size * n_sequences_per_stream
        sequences_per_batch = self.mini_batch_size

        indices = np.arange(num_total_sequences)
        np.random.shuffle(indices)

        for i in range(0, num_total_sequences, sequences_per_batch):
            batch_indices = indices[i : i + sequences_per_batch]
            batch = {}
            for key, tensor in all_data.items():
                sequences = tensor.reshape(num_total_sequences, self.sequence_length, *tensor.shape[2:])
                batch[key] = sequences[batch_indices].to(self.device)

            initial_h = batch['hidden_states'][:, 0, :]
            batch['initial_hidden_states'] = initial_h.unsqueeze(0)

            yield {
                key: tensor.transpose(0, 1) if key != 'initial_hidden_states' else tensor
                for key, tensor in batch.items()
            }
class ReplayBuffer:
    def __init__(self, obs_dim, action_dim, capacity, device):
        self.capacity = capacity
        self.device = device
        self.obs_buf = np.zeros((capacity, obs_dim), dtype=np.float32)
        self.next_obs_buf = np.zeros((capacity, obs_dim), dtype=np.float32)
        self.acts_buf = np.zeros((capacity, action_dim), dtype=np.float32)
        self.rews_buf = np.zeros((capacity, 1), dtype=np.float32)
        self.done_buf = np.zeros((capacity, 1), dtype=np.float32)
        self.ptr, self.size = 0, 0
    def add(self, obs, action, reward, next_obs, done):
        self.obs_buf[self.ptr] = obs
        self.acts_buf[self.ptr] = action
        self.rews_buf[self.ptr] = reward
        self.next_obs_buf[self.ptr] = next_obs
        self.done_buf[self.ptr] = done
        self.ptr = (self.ptr + 1) % self.capacity
        self.size = min(self.size + 1, self.capacity)
    def sample_batch(self, batch_size):
        idxs = np.random.randint(0, self.size, size=batch_size)
        batch = dict(
            obs=th.tensor(self.obs_buf[idxs], device=self.device),
            action=th.tensor(self.acts_buf[idxs], device=self.device),
            reward=th.tensor(self.rews_buf[idxs], device=self.device),
            next_obs=th.tensor(self.next_obs_buf[idxs], device=self.device),
            done=th.tensor(self.done_buf[idxs], device=self.device),
        )
        return batch