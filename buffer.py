import numpy as np
import torch
import random
from collections import deque


# ======================================================================================
# 1. í…ŒìŠ¤íŠ¸ ëŒ€ìƒ ë²„í¼ ì½”ë“œ (Canvasì— ìˆëŠ” ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜´)
# ======================================================================================
class RolloutBuffer:
    """
    ìˆœí™˜ ì‹ ê²½ë§(RNN/GRU) í•™ìŠµì„ ìœ„í•œ ë¡¤ì•„ì›ƒ ë²„í¼ (ë°°ì¹˜ ë°ì´í„° ì§€ì›).
    ë°ì´í„°ì˜ ì‹œê°„ì  ìˆœì„œë¥¼ ìœ ì§€í•˜ê³ , í•™ìŠµ ì‹œ ì—°ì†ëœ ì‹œí€€ìŠ¤ ë‹¨ìœ„ì˜ ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    def __init__(self,
                 obs_dim: int,
                 action_dim: int,
                 hidden_dim: int,
                 batch_size: int,              # ë¡¤ì•„ì›ƒ ì‹œ í•œ ìŠ¤í…ì— ì²˜ë¦¬í•˜ëŠ” ë°ì´í„°ì˜ ë°°ì¹˜ í¬ê¸°
                 sequence_length: int = 100,
                 n_steps: int = 100,
                 gamma: float = 0.99,
                 gae_lambda: float = 0.95,
                 mini_batch_size: int = 16,    # [ì˜ë¯¸ ë³€ê²½] í•œ ë¯¸ë‹ˆë°°ì¹˜ì— í¬í•¨ë  'ì‹œí€€ìŠ¤ì˜ ê°œìˆ˜'
                 device: torch.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")):

        self.n_steps = n_steps
        self.batch_size = batch_size
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.sequence_length = sequence_length
        self.mini_batch_size = mini_batch_size
        self.device = device
        self.gamma = gamma
        self.gae_lambda = gae_lambda

        self.reset()

    def reset(self):
        self.states = np.zeros((self.n_steps, self.batch_size, self.obs_dim), dtype=np.float32)
        # CartPoleì€ ì´ì‚° í–‰ë™ ê³µê°„ì´ë¯€ë¡œ actionì€ ìŠ¤ì¹¼ë¼, raw_actionì€ ë¡œì§“(2)
        self.raw_actions = np.zeros((self.n_steps, self.batch_size, self.action_dim), dtype=np.float32)
        self.actions = np.zeros((self.n_steps, self.batch_size, self.action_dim), dtype=np.float32)
        self.rewards = np.zeros((self.n_steps, self.batch_size), dtype=np.float32)
        self.dones = np.zeros((self.n_steps, self.batch_size), dtype=np.float32)
        self.log_probs = np.zeros((self.n_steps, self.batch_size), dtype=np.float32)
        self.values = np.zeros((self.n_steps, self.batch_size), dtype=np.float32)
        self.advantages = np.zeros((self.n_steps, self.batch_size), dtype=np.float32)
        self.returns = np.zeros((self.n_steps, self.batch_size), dtype=np.float32)
        self.hidden_states = np.zeros((self.n_steps, self.batch_size, self.hidden_dim), dtype=np.float32)
        self.ptr = 0
        self.full = False

    def add(self, state, raw_action, action, reward, done, log_prob, value, hidden_state):
        if self.ptr >= self.n_steps:
            self.full = True
            return

        self.states[self.ptr] = state
        self.raw_actions[self.ptr] = raw_action
        self.actions[self.ptr] = action
        self.rewards[self.ptr] = reward
        self.dones[self.ptr] = done
        self.log_probs[self.ptr] = log_prob
        self.values[self.ptr] = value.flatten()
        self.hidden_states[self.ptr] = hidden_state.detach().cpu().numpy().reshape(self.batch_size, self.hidden_dim)
        self.ptr = (self.ptr + 1)

    def compute_returns_and_advantages(self, last_values, last_dones):
        """GAE ê³„ì‚°. last_values/donesëŠ” (batch_size,) í˜•íƒœì˜ ë²¡í„°ì…ë‹ˆë‹¤."""
        # ğŸ”” [ìˆ˜ì •] ì…ë ¥ê°’ì´ ìŠ¤ì¹¼ë¼ì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ë°°ì—´ë¡œ ë³€í™˜í•˜ëŠ” ë¡œì§ ì¶”ê°€
        last_values = np.asarray(last_values).flatten()
        last_dones = np.asarray(last_dones).flatten()      
        last_gae_lam = 0
        for t in reversed(range(self.n_steps)):
            if t == self.n_steps - 1:
                next_non_terminal = 1.0 - last_dones
                next_values = last_values
            else:
                next_non_terminal = 1.0 - self.dones[t + 1]
                next_values = self.values[t + 1]

            delta = self.rewards[t] + self.gamma * next_values * next_non_terminal - self.values[t]
            last_gae_lam = delta + self.gamma * self.gae_lambda * next_non_terminal * last_gae_lam
            self.advantages[t] = last_gae_lam
        self.returns = self.advantages + self.values

    def __iter__(self):
        all_data = {
            'states': torch.tensor(self.states, dtype=torch.float32).swapaxes(0, 1),
            'raw_actions': torch.tensor(self.raw_actions, dtype=torch.float32).swapaxes(0, 1),
            'actions': torch.tensor(self.actions, dtype=torch.float32).swapaxes(0, 1),
            'log_probs': torch.tensor(self.log_probs, dtype=torch.float32).swapaxes(0, 1),
            'advantages': torch.tensor(self.advantages, dtype=torch.float32).swapaxes(0, 1),
            'returns': torch.tensor(self.returns, dtype=torch.float32).swapaxes(0, 1),
            'hidden_states': torch.tensor(self.hidden_states, dtype=torch.float32).swapaxes(0, 1),
        }

        n_sequences_per_stream = self.n_steps // self.sequence_length
        num_total_sequences = self.batch_size * n_sequences_per_stream
        sequences_per_batch = self.mini_batch_size

        indices = np.arange(num_total_sequences)
        np.random.shuffle(indices)

        for i in range(0, num_total_sequences, sequences_per_batch):
            batch_indices = indices[i : i + sequences_per_batch]
            batch = {}
            for key, tensor in all_data.items():
                sequences = tensor.reshape(num_total_sequences, self.sequence_length, *tensor.shape[2:])
                batch[key] = sequences[batch_indices].to(self.device)

            initial_h = batch['hidden_states'][:, 0, :]
            batch['initial_hidden_states'] = initial_h.unsqueeze(0)

            yield {
                key: tensor.transpose(0, 1) if key != 'initial_hidden_states' else tensor
                for key, tensor in batch.items()
            }


class ReplayBufferOffPolicy:
    """GRU ê¸°ë°˜ Off-Policy ì•Œê³ ë¦¬ì¦˜ì„ ìœ„í•œ ë¦¬í”Œë ˆì´ ë²„í¼ì…ë‹ˆë‹¤."""
    def __init__(self, capacity: int):
        self.buffer = deque(maxlen=capacity)

    def add(self, state, action, reward, next_state, done, hidden_state):
        """
        ë²„í¼ì— ê²½í—˜(transition)ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        ğŸ”” í–‰ë™ì„ ê²°ì •í•  ë•Œ ì‚¬ìš©ëœ hidden_stateë¥¼ í•¨ê»˜ ì €ì¥í•©ë‹ˆë‹¤.
        """
        # hidden_stateëŠ” GPUì— ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ CPUë¡œ ì˜®ê²¨ì„œ ì €ì¥
        hidden_state_cpu = hidden_state.detach().cpu().numpy()
        self.buffer.append((state, action, reward, next_state, done, hidden_state_cpu))

    def sample(self, batch_size: int) -> tuple:
        """ë²„í¼ì—ì„œ ë¬´ì‘ìœ„ë¡œ ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ìƒ˜í”Œë§í•©ë‹ˆë‹¤."""
        transitions = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones, hidden_states = zip(*transitions)
        
        return (np.array(states, dtype=np.float32),
                np.array(actions, dtype=np.float32),
                np.array(rewards, dtype=np.float32).reshape(-1, 1),
                np.array(next_states, dtype=np.float32),
                np.array(dones, dtype=np.float32).reshape(-1, 1),
                np.array(hidden_states, dtype=np.float32))

    def __len__(self) -> int:
        return len(self.buffer)
