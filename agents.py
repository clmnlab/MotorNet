import os
import sys 
import torch as th
import torch.nn as nn
import torch.optim as optim
import json
from pathlib import Path
from tqdm import tqdm
from torch.distributions import Normal
from utils import load_stuff
from utils import calculate_angles_between_vectors, calculate_lateral_deviation
from concurrent.futures import ProcessPoolExecutor, as_completed
from networks import GRUPolicy, ActorCriticGRU, GRUActor, GRUCritic, GRUSACActor
from buffer import RolloutBuffer, ReplayBufferOffPolicy
# from networks.pyì™€ buffer.pyì—ì„œ í•„ìš”í•œ í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
# ì´ ì½”ë“œëŠ” Soft Actor-Critic (SAC) ì—ì´ì „íŠ¸ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
# SAC ì—ì´ì „íŠ¸ëŠ” ì—°ì†ì ì¸ í–‰ë™ ê³µê°„ì„ ê°€ì§„ ê°•í™” í•™ìŠµ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
# ì´ ì—ì´ì „íŠ¸ëŠ” ì •ì±… ë„¤íŠ¸ì›Œí¬ì™€ Q-ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ í–‰ë™ì„ ì„ íƒí•˜ê³  í•™ìŠµí•©ë‹ˆë‹¤.
# ì´ ì½”ë“œëŠ” PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„ë˜ì—ˆìœ¼ë©°, GRU ê¸°ë°˜ì˜ ì •ì±… ë„¤íŠ¸ì›Œí¬ì™€ MLP ê¸°ë°˜ì˜ Q-ë„¤íŠ¸ì›Œí¬ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. 


class SLAgent:
    def __init__(self, obs_dim, action_dim, batch_size = 32, hidden_dims=128, device='cpu', lr=3e-4, 
                 loss_weight=None, gamma=0.99, tau=0.005, alpha=0.2,
                 freeze_output_layer=False, freeze_input_layer=False, learn_h0=True):
        self.device = device
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.batch_size = batch_size
        self.gamma = gamma
        self.tau = tau  # target update rate
        self.alpha = alpha  # entropy temperature
        self.network = GRUPolicy(obs_dim, hidden_dims, action_dim, device=device, 
                                       freeze_output_layer=freeze_output_layer, freeze_input_layer=freeze_input_layer).to(device)
        self.policy_optimizer = optim.Adam(self.network.parameters(), lr=lr)
        # self.replay_buffer = ReplayBuffer(obs_dim, action_dim, capacity=1000, device=device)        
        self.loss_weight = loss_weight if loss_weight is not None else np.array([1e+3,1e+5,1e-1,3e-4,1e-5,1e-3,0])
    def select_action(self, obs, h0=None): ## we can consider "deterministic" option
        h = self.network.init_hidden(batch_size=self.batch_size) if h0 is None else h0
        action, h = self.network(obs, h)
        return action, h
    
    def update(self, data):
        loss, _ = self.calc_loss(data, self.loss_weight)
        loss.to(self.device)
        self.policy_optimizer.zero_grad()
        loss.backward()
        self.policy_optimizer.step()
        return loss.item()
    
    def save(self, save_dir):
        th.save(self.network.state_dict(), f"{save_dir}")
        # print("done.")

    def load(self, load_dir):
        self.network.load_state_dict(th.load(f"{load_dir}"))
    
    def calc_loss(self, data, loss_weight):
        loss = {
            'position': None,
            'jerk': None,
            'muscle': None,
            'muscle_derivative': None,
            'hidden': None,
            'hidden_derivative': None,
            'hidden_jerk': None,}

        loss['position'] = th.mean(th.sum(th.abs(data['xy']-data['tg']), dim=-1),axis=1) # average over time not targets
        loss['jerk'] = th.mean(th.sum(th.square(th.diff(data['vel'], n=2, dim=1)), dim=-1))
        loss['muscle'] = th.mean(th.sum(data['all_force'], dim=-1))
        loss['muscle_derivative'] = th.mean(th.sum(th.square(th.diff(data['all_force'], n=1, dim=1)), dim=-1))
        loss['hidden'] = th.mean(th.square(data['all_hidden']))
        loss['hidden_derivative'] = th.mean(th.square(th.diff(data['all_hidden'], n=1, dim=1)))
        loss['hidden_jerk'] = th.mean(th.square(th.diff(data['all_hidden'], n=3, dim=1)))
        
        # if loss_weight is None:
        #     # currently in use
        #     loss_weight = np.array([1e+3,1e+5,1e-1,3e-4,1e-5,1e-3,0]) # 3.16227766e-04
            
        loss_weighted = {
            'position': loss_weight[0]*loss['position'],
            'jerk': loss_weight[1]*loss['jerk'],
            'muscle': loss_weight[2]*loss['muscle'],
            'muscle_derivative': loss_weight[3]*loss['muscle_derivative'],
            'hidden': loss_weight[4]*loss['hidden'],
            'hidden_derivative': loss_weight[5]*loss['hidden_derivative'],
            'hidden_jerk': loss_weight[6]*loss['hidden_jerk']
        }

        overall_loss = 0
        for key in loss_weighted:
            if key=='position':
                overall_loss += th.mean(loss_weighted[key].to(self.device))
            else:
                overall_loss += loss_weighted[key]

        return overall_loss, loss_weighted

 


class GRUPPOAgent:
    """ì‚¬ìš©ì ì •ì˜ PPO í•™ìŠµ ë¡œì§ì„ ë‹´ì€ ì—ì´ì „íŠ¸ í´ë˜ìŠ¤."""
    def __init__(self, env, 
                 hidden_dim=128, 
                 lr=1e-4, 
                 gamma=0.99, 
                 gae_lambda=0.95, 
                 clip_epsilon=0.2, 
                 n_epochs=10, 
                #  batch_size=64, 
                 sequence_length=100,
                 device='cuda'):
        self.env = env
        self.device = th.device(device)
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_epsilon = clip_epsilon
        self.n_epochs = n_epochs
        # self.batch_size = batch_size
        self.sequence_length = sequence_length
        obs_dim = env.observation_space.shape[0]
        action_dim = env.action_space.shape[0]

        self.network = ActorCriticGRU(obs_dim, action_dim, hidden_dim, device=self.device).to(self.device)
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)
    
    def select_action(self, obs, hidden_state):
        """í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•  ë•Œ í–‰ë™ì„ ì„ íƒí•©ë‹ˆë‹¤."""
        with th.no_grad():
            dist, value, new_hidden_state = self.network(obs, hidden_state)
            action_raw = dist.sample()
            action = th.sigmoid(action_raw)  # Sigmoidë¥¼ ì‚¬ìš©í•˜ì—¬ í–‰ë™ì„ [0, 1] ë²”ìœ„ë¡œ ì œí•œí•©ë‹ˆë‹¤.
            log_prob = dist.log_prob(action_raw).sum(axis=-1)
        
        # í™˜ê²½ ë° ë²„í¼ì™€ ìƒí˜¸ì‘ìš©í•˜ê¸° ìœ„í•´ numpy/scalar ê°’ìœ¼ë¡œ ë³€í™˜
        return (action.cpu().numpy(),
                action_raw.cpu().numpy(), # ì›ë³¸ í–‰ë™
                value.cpu().numpy(), 
                log_prob.cpu().numpy(), 
                new_hidden_state.detach())
        
    def update(self, buffer: RolloutBuffer):
        """ìˆ˜ì§‘ëœ ë°ì´í„°ë¡œ PPO ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        # ì–´ë“œë°´í‹°ì§€ ì •ê·œí™” (í•™ìŠµ ì•ˆì •ì„±ì— ì¤‘ìš”)
        advantages = th.tensor(buffer.advantages, dtype=th.float32).to(self.device)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        # ğŸ”” [ìˆ˜ì •] .detach()ë¥¼ ì¶”ê°€í•˜ì—¬ ê³„ì‚° ê·¸ë˜í”„ì—ì„œ ë¶„ë¦¬í•œ ë’¤ numpyë¡œ ë³€í™˜
        buffer.advantages = advantages.detach().cpu().numpy()
        
        for _ in range(self.n_epochs):
            # ğŸ”” [ë³€ê²½ì ] ë²„í¼ì˜ ì´í„°ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ê°„ë‹¨íˆ ê°€ì ¸ì˜´
            for batch in buffer:
                mb_states = batch['states']
                mb_raw_actions = batch['raw_actions']
                mb_old_log_probs = batch['log_probs']
                mb_advantages = batch['advantages']
                mb_returns = batch['returns']
                mb_h0 = batch['initial_hidden_states']

                h = mb_h0.contiguous()
                new_log_probs_list, values_list, entropy_list = [], [], []
                for t in range(self.sequence_length):
                    # të²ˆì§¸ ìŠ¤í…ì˜ ë°ì´í„° (mini_batch_size, feature_dim)
                    states_t = mb_states[t]
                    
                    # ì´ì „ ìŠ¤í…ì˜ ì€ë‹‰ ìƒíƒœ(h)ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
                    dist, values_t, h = self.network(states_t, h)

                    # ê³„ì‚°ëœ ê²°ê³¼ë“¤ì„ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
                    new_log_probs_list.append(dist.log_prob(mb_raw_actions[t]).sum(axis=-1))
                    values_list.append(values_t.flatten())
                    entropy_list.append(dist.entropy().mean())
                    
                # ë¦¬ìŠ¤íŠ¸ë“¤ì„ í•˜ë‚˜ì˜ í…ì„œë¡œ ê²°í•©
                # (sequence_length, mini_batch_size)
                new_log_probs = th.stack(new_log_probs_list)
                values = th.stack(values_list)
                entropy_loss = -th.stack(entropy_list).mean()

                # --- ì†ì‹¤ ê³„ì‚° ---
                # Value-function loss
                value_loss = nn.MSELoss()(values, mb_returns)
                
                # Policy-gradient loss (PPO-Clip)
                ratio = th.exp(new_log_probs - mb_old_log_probs)
                surr1 = ratio * mb_advantages
                surr2 = th.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * mb_advantages
                policy_loss = -th.min(surr1, surr2).mean()

                # ìµœì¢… ì†ì‹¤
                total_loss = policy_loss + 0.5 * value_loss + 0.01 * entropy_loss
                
                # ìµœì í™”
                self.optimizer.zero_grad()
                total_loss.backward()
                th.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=0.5) # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                self.optimizer.step()
        return policy_loss.item(), value_loss.item(), entropy_loss.item()
    
    def save(self, save_dir):
        th.save(self.network.state_dict(), f"{save_dir}")
        # print("done.")

    def load(self, load_dir):
        self.network.load_state_dict(th.load(f"{load_dir}"))


# ======================================================================================
#  GRUë¥¼ ì‚¬ìš©í•˜ëŠ” DDPG ì—ì´ì „íŠ¸
# ======================================================================================
class GRUDDPGAgent:
    def __init__(self, obs_dim, action_dim, hidden_dim, device, lr=1e-3, gamma=0.99, tau=0.005):
        self.device = device
        self.gamma = gamma
        self.tau = tau

        self.actor = GRUActor(obs_dim, action_dim, hidden_dim).to(device)
        self.actor_target = GRUActor(obs_dim, action_dim, hidden_dim).to(device)
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)

        self.critic = GRUCritic(obs_dim, action_dim, hidden_dim).to(device)
        self.critic_target = GRUCritic(obs_dim, action_dim, hidden_dim).to(device)
        self.critic_target.load_state_dict(self.critic.state_dict())
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)

    def select_action(self, obs, hidden_state):
        """í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•  ë•Œ í–‰ë™ê³¼ ë‹¤ìŒ ì€ë‹‰ ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        with th.no_grad():
            # obsì˜ ë°°ì¹˜ ì°¨ì›ì´ 1ì´ ì•„ë‹ ìˆ˜ ìˆìœ¼ë¯€ë¡œ unsqueeze ëŒ€ì‹  reshape ì‚¬ìš©
            if obs.ndim == 1:
                obs = obs.reshape(1, -1)
            obs_tensor = torch.tensor(obs, dtype=torch.float32).to(self.device)
            action, next_hidden_state = self.actor(obs_tensor, hidden_state)
        return action.cpu().numpy(), next_hidden_state

    def update(self, batch):
        states, actions, rewards, next_states, dones, hidden_states = [torch.tensor(b).to(self.device) for b in batch]
        
        # hidden_statesì˜ ì°¨ì›ì„ GRU ì…ë ¥ì— ë§ê²Œ ì¡°ì • (batch, 1, hidden_dim) -> (1, batch, hidden_dim)
        hidden_states = hidden_states.permute(1, 0, 2).contiguous()

        # --- í¬ë¦¬í‹± ì—…ë°ì´íŠ¸ ---
        with th.no_grad():
            # íƒ€ê²Ÿ ì•¡í„°ì— ë‹¤ìŒ ìƒíƒœì™€ 'í˜„ì¬' ì€ë‹‰ ìƒíƒœë¥¼ ë„£ì–´ ë‹¤ìŒ í–‰ë™ ì˜ˆì¸¡
            next_actions, _ = self.actor_target(next_states, hidden_states)
            target_q = self.critic_target(next_states, next_actions, hidden_states)
            target_q = rewards + self.gamma * (1 - dones) * target_q
        
        current_q = self.critic(states, actions, hidden_states)
        critic_loss = F.mse_loss(current_q, target_q)
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # --- ì•¡í„° ì—…ë°ì´íŠ¸ ---
        # ì•¡í„°ê°€ ìƒì„±í•œ í–‰ë™ì„ í¬ë¦¬í‹±ì´ í‰ê°€
        new_actions, _ = self.actor(states, hidden_states)
        actor_loss = -self.critic(states, new_actions, hidden_states).mean()
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        # --- íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì†Œí”„íŠ¸ ì—…ë°ì´íŠ¸ ---
        self._soft_update(self.actor_target, self.actor)
        self._soft_update(self.critic_target, self.critic)

        return critic_loss.item(), actor_loss.item()

    def _soft_update(self, target, source):
        for target_param, source_param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(self.tau * source_param.data + (1.0 - self.tau) * target_param.data)

# ======================================================================================
# GRUë¥¼ ì‚¬ìš©í•˜ëŠ” SAC ì—ì´ì „íŠ¸
# ======================================================================================
class GRUSACAgent:
    def __init__(self, obs_dim, action_dim, hidden_dim, device, lr=3e-4, gamma=0.99, tau=0.005):
        self.device = device
        self.gamma = gamma
        self.tau = tau

        self.actor = GRUSACActor(obs_dim, action_dim, hidden_dim).to(device)
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)

        self.critic1 = GRUCritic(obs_dim, action_dim, hidden_dim).to(device)
        self.critic1_target = GRUCritic(obs_dim, action_dim, hidden_dim).to(device)
        self.critic1_target.load_state_dict(self.critic1.state_dict())
        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=lr)

        self.critic2 = GRUCritic(obs_dim, action_dim, hidden_dim).to(device)
        self.critic2_target = GRUCritic(obs_dim, action_dim, hidden_dim).to(device)
        self.cri