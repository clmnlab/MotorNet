import torch as th
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Normal
import networks


from networks import GRUPolicy, ActorCriticGRU, QNetwork
from buffer import ReplayBuffer, RolloutBuffer
# from networks.pyì™€ buffer.pyì—ì„œ í•„ìš”í•œ í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
# ì´ ì½”ë“œëŠ” Soft Actor-Critic (SAC) ì—ì´ì „íŠ¸ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
# SAC ì—ì´ì „íŠ¸ëŠ” ì—°ì†ì ì¸ í–‰ë™ ê³µê°„ì„ ê°€ì§„ ê°•í™” í•™ìŠµ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
# ì´ ì—ì´ì „íŠ¸ëŠ” ì •ì±… ë„¤íŠ¸ì›Œí¬ì™€ Q-ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ í–‰ë™ì„ ì„ íƒí•˜ê³  í•™ìŠµí•©ë‹ˆë‹¤.
# ì´ ì½”ë“œëŠ” PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„ë˜ì—ˆìœ¼ë©°, GRU ê¸°ë°˜ì˜ ì •ì±… ë„¤íŠ¸ì›Œí¬ì™€ MLP ê¸°ë°˜ì˜ Q-ë„¤íŠ¸ì›Œí¬ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. 

import os
import sys 
from utils import load_stuff
from utils import calculate_angles_between_vectors, calculate_lateral_deviation
import torch as th
import numpy as np
import json
from pathlib import Path
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed


class SLAgent:
    def __init__(self, obs_dim, action_dim, batch_size = 32, hidden_dims=128, device='cpu', lr=3e-4, 
                 loss_weight=None, gamma=0.99, tau=0.005, alpha=0.2,
                 freeze_output_layer=False, freeze_input_layer=False, learn_h0=True):
        self.device = device
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.batch_size = batch_size
        self.gamma = gamma
        self.tau = tau  # target update rate
        self.alpha = alpha  # entropy temperature
        self.network = GRUPolicy(obs_dim, hidden_dims, action_dim, device=device, 
                                       freeze_output_layer=freeze_output_layer, freeze_input_layer=freeze_input_layer).to(device)
        self.policy_optimizer = optim.Adam(self.network.parameters(), lr=lr)
        self.replay_buffer = ReplayBuffer(obs_dim, action_dim, capacity=1000, device=device)        
        self.loss_weight = loss_weight if loss_weight is not None else np.array([1e+3,1e+5,1e-1,3e-4,1e-5,1e-3,0])
    def select_action(self, obs, h0=None): ## we can consider "deterministic" option
        h = self.network.init_hidden(batch_size=self.batch_size) if h0 is None else h0
        action, h = self.network(obs, h)
        return action, h
    
    def update(self, data):
        loss, _ = self.calc_loss(data, self.loss_weight)
        loss.to(self.device)
        self.policy_optimizer.zero_grad()
        loss.backward()
        self.policy_optimizer.step()
        return loss.item()
    
    def save(self, save_dir):
        th.save(self.network.state_dict(), f"{save_dir}")
        print("done.")

    def load(self, load_dir):
        self.network.load_state_dict(th.load(f"{load_dir}"))
    
    def calc_loss(self, data, loss_weight):
        loss = {
            'position': None,
            'jerk': None,
            'muscle': None,
            'muscle_derivative': None,
            'hidden': None,
            'hidden_derivative': None,
            'hidden_jerk': None,}

        loss['position'] = th.mean(th.sum(th.abs(data['xy']-data['tg']), dim=-1),axis=1) # average over time not targets
        loss['jerk'] = th.mean(th.sum(th.square(th.diff(data['vel'], n=2, dim=1)), dim=-1))
        loss['muscle'] = th.mean(th.sum(data['all_force'], dim=-1))
        loss['muscle_derivative'] = th.mean(th.sum(th.square(th.diff(data['all_force'], n=1, dim=1)), dim=-1))
        loss['hidden'] = th.mean(th.square(data['all_hidden']))
        loss['hidden_derivative'] = th.mean(th.square(th.diff(data['all_hidden'], n=1, dim=1)))
        loss['hidden_jerk'] = th.mean(th.square(th.diff(data['all_hidden'], n=3, dim=1)))
        
        # if loss_weight is None:
        #     # currently in use
        #     loss_weight = np.array([1e+3,1e+5,1e-1,3e-4,1e-5,1e-3,0]) # 3.16227766e-04
            
        loss_weighted = {
            'position': loss_weight[0]*loss['position'],
            'jerk': loss_weight[1]*loss['jerk'],
            'muscle': loss_weight[2]*loss['muscle'],
            'muscle_derivative': loss_weight[3]*loss['muscle_derivative'],
            'hidden': loss_weight[4]*loss['hidden'],
            'hidden_derivative': loss_weight[5]*loss['hidden_derivative'],
            'hidden_jerk': loss_weight[6]*loss['hidden_jerk']
        }

        overall_loss = 0
        for key in loss_weighted:
            if key=='position':
                overall_loss += th.mean(loss_weighted[key].to(self.device))
            else:
                overall_loss += loss_weighted[key]

        return overall_loss, loss_weighted

 


class GRUPPOAgent:
    """ì‚¬ìš©ì ì •ì˜ PPO í•™ìŠµ ë¡œì§ì„ ë‹´ì€ ì—ì´ì „íŠ¸ í´ë˜ìŠ¤."""
    def __init__(self, env, hidden_dim=128, lr=1e-4, gamma=0.99, gae_lambda=0.95, clip_epsilon=0.2, n_epochs=10, batch_size=64, device='cuda'):
        self.env = env
        self.device = th.device(device)
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_epsilon = clip_epsilon
        self.n_epochs = n_epochs
        self.batch_size = batch_size
        
        obs_dim = env.observation_space.shape[0]
        action_dim = env.action_space.shape[0]

        self.network = ActorCriticGRU(obs_dim, action_dim, hidden_dim, device=self.device).to(self.device)
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)
    
    def select_action(self, obs, hidden_state):
        """í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•  ë•Œ í–‰ë™ì„ ì„ íƒí•©ë‹ˆë‹¤."""
        with th.no_grad():
            dist, value, new_hidden_state = self.network(obs, hidden_state)
            action_raw = dist.sample()
            action = th.sigmoid(action_raw)  # Sigmoidë¥¼ ì‚¬ìš©í•˜ì—¬ í–‰ë™ì„ [0, 1] ë²”ìœ„ë¡œ ì œí•œí•©ë‹ˆë‹¤.
            log_prob = dist.log_prob(action_raw).sum(axis=-1)
        
        # í™˜ê²½ ë° ë²„í¼ì™€ ìƒí˜¸ì‘ìš©í•˜ê¸° ìœ„í•´ numpy/scalar ê°’ìœ¼ë¡œ ë³€í™˜
        return (action.cpu().numpy(),
                action_raw.cpu().numpy(), # ì›ë³¸ í–‰ë™
                value.cpu().numpy(), 
                log_prob.cpu().numpy(), 
                new_hidden_state.detach())
        
    def update(self, buffer: RolloutBuffer):
        """ìˆ˜ì§‘ëœ ë°ì´í„°ë¡œ PPO ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        # ë²„í¼ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ í‰íƒ„í™”í•˜ì—¬ ê°€ì ¸ì˜´ (ì´í„°ë ˆì´í„° ë‚´ë¶€ ë¡œì§ê³¼ ìœ ì‚¬)
        all_advantages = buffer.advantages.reshape(-1)
        advantages_tensor = th.tensor(all_advantages, dtype=th.float32).to(self.device)
        advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-8)        
        # ì´í„°ë ˆì´í„°ì—ì„œ ì •ê·œí™”ëœ advantagesë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë‹¤ì‹œ ì›ë˜ ëª¨ì–‘ìœ¼ë¡œ ë§Œë“¦
        buffer.advantages = advantages_tensor.cpu().numpy().reshape(buffer.n_steps, buffer.batch_size)
        
        for _ in range(self.n_epochs):
            # ğŸ”” [ë³€ê²½ì ] ë²„í¼ì˜ ì´í„°ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ê°„ë‹¨íˆ ê°€ì ¸ì˜´
            for batch in buffer:
                mb_states = batch['states']
                mb_raw_actions = batch['raw_actions']
                mb_old_log_probs = batch['log_probs']
                mb_advantages = batch['advantages']
                mb_returns = batch['returns']

                # âš ï¸ GRU í•œê³„: ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ì¸í•´ ì€ë‹‰ ìƒíƒœëŠ” ë§¤ë²ˆ ì´ˆê¸°í™”
                h0 = self.network.init_hidden(mb_states.shape[0])
                dist, values, _ = self.network(mb_states, h0)
                
                # ì†ì‹¤ ê³„ì‚°
                value_loss = nn.MSELoss()(values.squeeze(), mb_returns)
                
                new_log_probs = dist.log_prob(mb_raw_actions).sum(axis=-1)
                ratio = th.exp(new_log_probs - mb_old_log_probs)
                
                surr1 = ratio * mb_advantages
                surr2 = th.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * mb_advantages
                policy_loss = -th.min(surr1, surr2).mean()
                
                entropy_loss = -dist.entropy().mean()
                
                total_loss = policy_loss + 0.5*value_loss + 0.01 * entropy_loss
                
                # ìµœì í™”
                self.optimizer.zero_grad()
                total_loss.backward()
                # th.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=0.5) # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                self.optimizer.step()
        return policy_loss.item(), value_loss.item(), entropy_loss.item()
    
    def save(self, save_dir):
        th.save(self.network.state_dict(), f"{save_dir}")
        print("done.")

    def load(self, load_dir):
        self.network.load_state_dict(th.load(f"{load_dir}"))



class SACAgent:
    def __init__(self, obs_dim, action_dim, hidden_dims, device,
                 lr=3e-4, gamma=0.99, tau=0.005, alpha=0.2):
        self.device = device
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.tau = tau  # target update rate
        self.alpha = alpha  # entropy temperature

        # Networks
        self.policy_net = GRUPolicy(obs_dim, hidden_dims, action_dim, device=device, 
                                       freeze_output_layer=freeze_output_layer, freeze_input_layer=freeze_input_layer).to(device)
        self.q1_net = QNetwork(obs_dim, action_dim, hidden_dims).to(device)
        self.q2_net = QNetwork(obs_dim, action_dim, hidden_dims).to(device)
        self.q1_target = QNetwork(obs_dim, action_dim, hidden_dims).to(device)
        self.q2_target = QNetwork(obs_dim, action_dim, hidden_dims).to(device)
        # copy weights
        self.q1_target.load_state_dict(self.q1_net.state_dict())
        self.q2_target.load_state_dict(self.q2_net.state_dict())

        # Optimizers
        self.pi_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)
        self.q1_optimizer = optim.Adam(self.q1_net.parameters(), lr=lr)
        self.q2_optimizer = optim.Adam(self.q2_net.parameters(), lr=lr)
        # Optional: alpha ìë™ ì¡°ì • ê¸°ëŠ¥ ë“± ì¶”ê°€ ê°€ëŠ¥

    def select_action(self, obs, h0=None, deterministic=False):
        # obs: torch.Tensor (obs_dim,) or (batch, obs_dim)
        obs_t = obs.unsqueeze(0) if obs.ndim == 1 else obs
        mean, log_std, h1 = self.policy_net(obs_t, h0)
        std = th.exp(log_std)
        if deterministic:
            action = mean
            log_prob = None
        else:
            dist = Normal(mean, std)
            x = dist.rsample()
            log_prob = dist.log_prob(x).sum(dim=-1, keepdim=True)
            action = x
        # í•„ìš”í•œ ë²”ìœ„ë¡œ clamp í˜¹ì€ tanh í›„ log_prob ë³´ì •
        return action.squeeze(0), log_prob, h1

    def update(self, replay_buffer, batch_size):
        # ìƒ˜í”Œ ë°°ì¹˜
        batch = replay_buffer.sample_batch(batch_size)
        obs = batch['obs']       # (B, obs_dim)
        action = batch['action'] # (B, action_dim)
        reward = batch['reward'] # (B,1)
        next_obs = batch['next_obs']
        done = batch['done']     # (B,1)

        # 1) Q-network ì—…ë°ì´íŠ¸
        with th.no_grad():
            # ë‹¤ìŒ í–‰ë™ ìƒ˜í”Œë§
            next_mean, next_log_std, _ = self.policy_net(next_obs)
            next_std = th.exp(next_log_std)
            next_dist = Normal(next_mean, next_std)
            next_action = next_dist.rsample()
            next_log_prob = next_dist.log_prob(next_action).sum(dim=-1, keepdim=True)
            # íƒ€ê¹ƒ Q ê°’
            q1_targ = self.q1_target(next_obs, next_action)
            q2_targ = self.q2_target(next_obs, next_action)
            q_targ_min = th.min(q1_targ, q2_targ)
            target_Q = reward + self.gamma * (1 - done) * (q_targ_min - self.alpha * next_log_prob)
        # í˜„ì¬ Q ì˜ˆì¸¡
        q1_pred = self.q1_net(obs, action)
        q2_pred = self.q2_net(obs, action)
        q1_loss = nn.MSELoss()(q1_pred, target_Q)
        q2_loss = nn.MSELoss()(q2_pred, target_Q)
        self.q1_optimizer.zero_grad(); q1_loss.backward(); self.q1_optimizer.step()
        self.q2_optimizer.zero_grad(); q2_loss.backward(); self.q2_optimizer.step()

        # 2) Policy ì—…ë°ì´íŠ¸
        mean, log_std, _ = self.policy_net(obs)
        std = th.exp(log_std)
        dist = Normal(mean, std)
        new_action = dist.rsample()
        log_prob = dist.log_prob(new_action).sum(dim=-1, keepdim=True)
        q1_new = self.q1_net(obs, new_action)
        q2_new = self.q2_net(obs, new_action)
        q_new_min = th.min(q1_new, q2_new)
        policy_loss = (self.alpha * log_prob - q_new_min).mean()
        self.pi_optimizer.zero_grad(); policy_loss.backward(); self.pi_optimizer.step()

        # 3) Target network soft update
        for param, target_param in zip(self.q1_net.parameters(), self.q1_target.parameters()):
            target_param.data.copy_( self.tau * param.data + (1 - self.tau) * target_param.data )
        for param, target_param in zip(self.q2_net.parameters(), self.q2_target.parameters()):
            target_param.data.copy_( self.tau * param.data + (1 - self.tau) * target_param.data )

        return {
            'q1_loss': q1_loss.item(),
            'q2_loss': q2_loss.item(),
            'policy_loss': policy_loss.item()
        }

    def save(self, save_dir):
        th.save(self.policy_net.state_dict(), f"{save_dir}/policy.pth")
        th.save(self.q1_net.state_dict(), f"{save_dir}/q1.pth")
        th.save(self.q2_net.state_dict(), f"{save_dir}/q2.pth")
        # íƒ€ê¹ƒ ë„¤íŠ¸ì›Œí¬ëŠ” í•™ìŠµ ì‹œ ë§¤ë²ˆ ë®ì–´ì“°ë¯€ë¡œ ë³„ë„ ì €ì¥ í•„ìš” ì—†ìŒ

    def load(self, load_dir):
        self.policy_net.load_state_dict(th.load(f"{load_dir}/policy.pth"))
        self.q1_net.load_state_dict(th.load(f"{load_dir}/q1.pth"))
        self.q2_net.load_state_dict(th.load(f"{load_dir}/q2.pth"))
        # íƒ€ê¹ƒ ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self.q1_target.load_state_dict(self.q1_net.state_dict())
        self.q2_target.load_state_dict(self.q2_net.state_dict())
